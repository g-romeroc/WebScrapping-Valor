{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a39fa2",
   "metadata": {},
   "source": [
    "## WebScrapping - Valor Econômico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "# Novas\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41237a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('--headless')\n",
    "navegador = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac156",
   "metadata": {},
   "source": [
    "### Parte 1: Extração das URLs Diárias de Publicações do Valor Econômico\n",
    "\n",
    "Este segmento do processo envolve a análise do 'sitemap.xml' do Valor Econômico para identificar e coletar as URLs que correspondem às edições diárias. Cada uma dessas URLs contém os links para os artigos publicados em um dia específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2615f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sitemap = 'https://valor.globo.com/sitemap/valor/sitemap.xml'\n",
    "navegador.get(sitemap)\n",
    "sleep(1)\n",
    "site = BeautifulSoup(navegador.page_source, 'xml')\n",
    "\n",
    "links_sitemap = []\n",
    "\n",
    "links = site.findAll('loc')\n",
    "\n",
    "for link in links:\n",
    "    \n",
    "    url = link.text\n",
    "    \n",
    "    links_sitemap.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99d834",
   "metadata": {},
   "source": [
    "### Parte 2: Compilação das Notícias Diárias do Valor Econômico\n",
    "\n",
    "Nesta etapa, consolidamos todas as notícias acessando as URLs das edições diárias previamente identificadas, coletando assim o conteúdo publicado para cada dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "links_noticias = []\n",
    "\n",
    "for i in tqdm(range(0,len(links_sitemap))):\n",
    "    navegador.get(links_sitemap[i])\n",
    "    sleep(1)\n",
    "    site = BeautifulSoup(navegador.page_source, 'xml')\n",
    "    \n",
    "    noticias = site.findAll('url')\n",
    "    \n",
    "    for noticia in noticias:\n",
    "        \n",
    "        html = noticia.find('loc').text\n",
    "        links_noticias.append(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b211db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de Notícias\n",
    "len(links_noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando em DataFrame\n",
    "df_links = pd.DataFrame(links_noticias)\n",
    "df_links.to_csv('LN_Valor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85759da1",
   "metadata": {},
   "source": [
    "### Parte 3: Login Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37870774",
   "metadata": {},
   "outputs": [],
   "source": [
    "navegador.get('https://login.globo.com/login/6908/connect-confirm?url=https%3A%2F%2Fid.globo.com%2Fauth%2Frealms%2Fglobo.com%2Flogin-actions%2Fauthenticate%3Fsession_code%3Dc9e54WgBYYO_tXm-neZ-sC_uKKfpxgaJz0XC2Xer_j0%26execution%3Db5dd88dc-447e-468f-945e-e7c7de4883b7%26client_id%3Dvalor%2540globoid-connect%26tab_id%3D9DJDno0akOQ%26request-context%3DcluCi4&error=&request-context=cluCi4')\n",
    "sleep(1)\n",
    "\n",
    "email = 'seu-email'\n",
    "senha = 'sua senha'\n",
    "\n",
    "\n",
    "navegador.find_element_by_name(\"login\").send_keys(email)\n",
    "sleep(1)\n",
    "navegador.find_element_by_name(\"password\").send_keys(senha)\n",
    "sleep(1)\n",
    "\n",
    "\n",
    "navegador.find_element_by_css_selector('#login-form > div.actions > button').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8382bd",
   "metadata": {},
   "source": [
    "### Parte 4: Texto, Título e Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv('LN_Valor.csv')\n",
    "news_links = df_links['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando os links que possuem o termo 'ao-vivo', já que se tratam de coberturas.\n",
    "news_links = [item for item in news_links if 'ao-vivo' not in item]\n",
    "# Tirando os links patrocinados\n",
    "news_links = [item for item in news_links if 'patrocinado' not in item]\n",
    "# Retirando os Stories\n",
    "news_links = [item for item in news_links if 'stories' not in item]\n",
    "len(news_links) # Novo número de Notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirar páginas relacionadas a 'eu-e'\n",
    "ok = [item for item in news_links if 'eu-e' not in item]\n",
    "# Retirar páginas relacionadas a carreiras\n",
    "ok1 = [item for item in ok if 'carreira' not in item]\n",
    "# Retirar páginas relacionadas a 'especiais'\n",
    "ok3 = [item for item in ok1 if 'especiais' not in item]\n",
    "len(ok3) # Novo número de Notícias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d33a",
   "metadata": {},
   "source": [
    "# Extração dos Dados das Publicações\n",
    "\n",
    "Ajuste a variável j de acordo com a capacidade de memória e processamento do seu computador. Atualmente, o script está configurado para extrair dados de 10.000 notícias por sessão, mas este valor pode ser modificado conforme necessário. Dependendo do volume de dados, pode ser preciso dividir o processo de raspagem de dados em várias etapas. Além disso, para minimizar o risco de bloqueio por parte do site do Valor Econômico devido a atividades de raspagem intensiva, foi implementado um intervalo de espera (sleep) de 0.6 segundos antes de cada coleta de dados após acessar a página. Pórem, esse valor ainda pode ser insuficiente em alguns momentos. \n",
    "\n",
    "Caso a coleta de dados não seja bem-sucedida para algumas notícias durante a sessão de extração de 10.000 artigos, os respectivos links serão armazenados em um dataframe(**Erro_p{j}.csv**). Este dataframe estará disponível para que possamos retomar e concluir a raspagem de dados dos artigos que inicialmente encontraram erros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_com_erro=list()\n",
    "erros = list()\n",
    "\n",
    "for j in range(0,1):\n",
    "    news_1 = []\n",
    "\n",
    "    for i in tqdm(range(j*10000,(j+1)*10000)):\n",
    "        try:\n",
    "            url = news_links[i]\n",
    "            navegador.get(url)\n",
    "\n",
    "            sleep(0.6)\n",
    "            site = BeautifulSoup(navegador.page_source, 'html.parser')\n",
    "\n",
    "            titulo = site.find('h1', attrs={'class':'content-head__title'}).text\n",
    "            data = site.find('time', attrs={'itemprop':'datePublished'}).text.replace('h',':')\n",
    "            data1 = datetime.strptime(data,' %d/%m/%Y %H:%M ')\n",
    "\n",
    "            article_text = ''\n",
    "            article = site.find('article', attrs={'itemprop':'articleBody'}).findAll('div', attrs={'class':'mc-column content-text active-extra-styles'})\n",
    "            if article:\n",
    "                for element in article:\n",
    "                    article_text += ''.join(element.text)\n",
    "            else:\n",
    "                article_text = site.find('article', attrs={'itemprop':'articleBody'}).text\n",
    "                \n",
    "            news_1.append([data1, titulo, article_text, url])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            links_com_erro.append(url)\n",
    "                    \n",
    "\n",
    "    valor_p1 = pd.DataFrame(news_1, columns=['Data', 'Título', 'Texto', 'Url'])\n",
    "    valor_p1.to_csv(f'Valor_p{j}.csv', index=False)\n",
    "    erro_p1 = pd.DataFrame(links_com_erro)\n",
    "    erro_p1.to_csv(f'Erro_p{j}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
